{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583a5e76-0d4d-4df1-94ba-5e232bd1c0e8",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d035a86-bdd7-4b40-8d01-d6f2bd66d6f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/20 02:35:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d6ac5-8a75-4abe-9846-c36ae8597032",
   "metadata": {},
   "source": [
    "## Use `map()` on pyspark dataframe.rdd\n",
    "\n",
    "*Should avoid rdd as much as possible*\n",
    "\n",
    "[rdd.map func](https://sparkbyexamples.com/pyspark/pyspark-map-transformation/)\n",
    "\n",
    "DataFrame doesnâ€™t have map() transformation to use with DataFrame hence you need to DataFrame to RDD first:\n",
    "\n",
    "```python\n",
    "# rdd \n",
    "sparkDataframe.rdd\n",
    "```\n",
    "\n",
    "> PySpark `map()` is a RDD transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD. \n",
    "\n",
    "It is used to apply any complex operations like adding a column, updating a column, transforming the data etc.\n",
    "\n",
    "**Note:** You have to list out all the columns in expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef28e24e-7bd4-4b09-9473-1fb96c78abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| john| 33|\n",
      "| jack| 26|\n",
      "|derry| 28|\n",
      "| mary| 64|\n",
      "+-----+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"name\", StringType(), True),\n",
    "                    StructField(\"age\", IntegerType(), True)])\n",
    "textFile = spark.read.csv(\"sample.csv\", header=True, schema=schema)\n",
    "textFile.show()\n",
    "textFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41368fda-7874-43ac-81b8-7206e933b33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def funcX(rdd):\n",
    "    return (rdd.name + \"!\", rdd.age + 1)\n",
    "\n",
    "# use function\n",
    "lambdaCase1 = textFile.rdd.map(lambda x : funcX(x)).collect()\n",
    "\n",
    "# or all in expression\n",
    "lambdaCase2 = textFile.rdd.map(lambda x : (x['name']+\"!\", x['age'] + 1)).collect()\n",
    "lambdaCase1 == lambdaCase2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7ae33-a641-4f65-9844-890af27ba5dc",
   "metadata": {},
   "source": [
    "### After `map()` - get list or pyspark dataframe\n",
    "\n",
    "To list by `collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df2ec657-7bad-4471-b585-268f70f42091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 34), ('jack', 27), ('derry', 29), ('mary', 65)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = textFile.rdd.map(lambda x : funcX(x)).collect()\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257fb05-1f37-426e-8da5-c01b57cd8e61",
   "metadata": {},
   "source": [
    "Back to dataframe by `toDF(schema) | toDF([columns])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3baf6372-83b5-4f83-aa5e-1d93d0933538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = textFile.rdd.map(lambda x : (x.name + \"!\", x.age + 1)).toDF(schema)\n",
    "rs.printSchema()\n",
    "type(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69532d76-97be-49cb-ae8a-0a7831e52e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
