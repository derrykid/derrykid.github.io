{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fee13f-7f2b-4abf-8c7b-0b84c83733ae",
   "metadata": {},
   "source": [
    "# Apache Arrow\n",
    "\n",
    "> Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes. This currently is most beneficial to Python users that work with Pandas/NumPy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e416c95-656d-4d14-8185-bdf8c7cb7b78",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "```bash\n",
    "pip install pyspark[sql]\n",
    "```\n",
    "\n",
    "Users need to first set the Spark configuration `spark.sql.execution.arrow.pyspark.enabled` to true. This is disabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7371fb1-ae9b-4e65-849f-329add67f2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/20 23:04:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf8da3-e487-4095-ae1b-da97743e84ef",
   "metadata": {},
   "source": [
    "## Convert to spark dataframe / pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72292bf-e752-4b59-8906-97c85dd8f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derry/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b251731-b3a1-4d48-a8ff-73ac72e8aebf",
   "metadata": {},
   "source": [
    "## Pandas user defined function\n",
    "\n",
    "[doc](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html#pyspark.sql.functions.pandas_udf)\n",
    "\n",
    "imports:\n",
    "```python\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "Syntax\n",
    "```python\n",
    "pandas_udf(f=None, returnType=None, functionType=None)\n",
    "```\n",
    "- `f` - User defined function\n",
    "- `returnType` - This is optional but when specified it should be either a DDL-formatted type string or any type of pyspark.sql.types.DataType\n",
    "- `functionType` - int, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ab20d1-92ab-47a6-bb86-4471d28e46d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ee7d37-4493-4539-b002-d721ba095fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+\n",
      "|Seqno|        Name|   UpperCase|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def to_upper(series : pd.Series) -> pd.Series:\n",
    "    return series.str.upper()\n",
    "\n",
    "df.withColumn(\"UpperCase\", to_upper(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187ed49-4d32-4990-be37-2adb4c8164be",
   "metadata": {},
   "source": [
    "## Use `pyspark.pandas` apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff7ff9a-53a0-4128-a9c3-a92e5e666125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "/home/derry/.local/lib/python3.10/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/home/derry/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Fee  Discount\n",
      "0  20000.0      1000\n",
      "1  25000.0      2500\n",
      "2  30000.0      1500\n",
      "3  22000.0      1200\n",
      "4      NaN      3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derry/.local/lib/python3.10/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/home/derry/.local/lib/python3.10/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/home/derry/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21000.0\n",
      "1    27500.0\n",
      "2    31500.0\n",
      "3    23200.0\n",
      "4        NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "technologies = ({'Fee' :[20000,25000,30000,22000,np.NaN],\n",
    "                'Discount':[1000,2500,1500,1200,3000]})\n",
    "\n",
    "psdf = ps.DataFrame(technologies)\n",
    "print(psdf)\n",
    "\n",
    "def add(data):\n",
    "   return data[0]+data[1]\n",
    "\n",
    "# Apply the function to DataFrame   \n",
    "addDF = psdf.apply(add, axis=1)\n",
    "print(addDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc10d3-a8a0-4d86-a7e0-394156d196fa",
   "metadata": {},
   "source": [
    "## use DDL in return type when input/output StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "500b8ef3-3406-40e1-b0b3-c944014f5c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "            #struct DDL string\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a struct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "194855d4-2fa3-43ea-ad21-063b582f8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|split_expand(name)|\n",
      "+------------------+\n",
      "|       {John, Doe}|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"first string, last string\")\n",
    "def split_expand(s: pd.Series) -> pd.DataFrame:\n",
    "    return s.str.split(expand=True)\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.select(split_expand(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019a2dd-3b93-4a4b-8324-c98db3bfc66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
