{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d73448a-7e06-4da5-be92-13fedb174d15",
   "metadata": {},
   "source": [
    "# functions\n",
    "\n",
    "[aggregate functions](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions)\n",
    "\n",
    "[groupBy function](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example)\n",
    "\n",
    "# window function\n",
    "\n",
    "[window](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/968100988546031/157591980591166/8836542754149149/latest.html)\n",
    "\n",
    "[examples](https://sparkbyexamples.com/pyspark/pyspark-window-functions/)\n",
    "> Pyspark window functions are useful when you want to examine relationships within groups of data rather than between groups of data (as for groupBy)\n",
    "\n",
    "# literal\n",
    "\n",
    "[lit()](https://sparkbyexamples.com/pyspark/pyspark-lit-add-literal-constant/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ff131-21d9-4356-bf88-d8c09967a7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
